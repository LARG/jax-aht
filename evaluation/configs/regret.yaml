defaults:
  - task: lbf # overcooked/cramped_room # task configs
  - hydra: hydra_simple
  - _self_

# TODO: restructure this config to use per task and per-algorithm configs!
ENV_NAME: ${task.ENV_NAME}
ENV_KWARGS: ${task.ENV_KWARGS}
ROLLOUT_LENGTH: ${task.ROLLOUT_LENGTH}
TASK_NAME: ${task.TASK_NAME}

ego_agent:
  path: "results_explore/lbf/ppo_ego_s5/2025-04-13_11-43-33/saved_train_run" # results/lbf/fcp_s5/2025-04-13_18-42-46/saved_train_run
  actor_type: s5
  idx_list: [[0, -1]] # list of idxs to load from checkpoint for evaluation. If null, all checkpoints will be loaded.
  test_mode: true
  # provide any necessary args to initialize the agent here
  # ex) S5_ACTOR_CRITIC_HIDDEN_DIM: 64 
  # defaults for each actor type will be used if no args are provided

algorithm:
  ALG: regret_maximizing
  TOTAL_TIMESTEPS: 3e6 # 1.6e7 # 3e6
  PARTNER_POP_SIZE: 1 # how many regret-maximizing pairs to train
  CONF_BR_WEIGHT: 0.5 # conf-ego weight set to 1 - conf-br weight
  TRAIN_SEED: 38410
  NUM_ENVS: 16
  ENV_NAME: ${ENV_NAME}
  ENV_KWARGS: ${ENV_KWARGS}
  ROLLOUT_LENGTH: ${ROLLOUT_LENGTH}
  LR: 1.e-4
  UPDATE_EPOCHS: 15
  NUM_MINIBATCHES: 4
  NUM_EVAL_EPISODES: 20
  NUM_CHECKPOINTS: 5
  GAMMA: 0.99
  GAE_LAMBDA: 0.95
  CLIP_EPS: 0.05
  ENT_COEF: 0.001 # TODO: check if this should be larger?
  VF_COEF: 1.0
  MAX_GRAD_NORM: 1.0
  ANNEAL_LR: false

label: ""
name: ${TASK_NAME}/regret_maximizing/${label}

# wandb settings
logger: 
  project: open-ended-aht
  entity: aht-project
  tags: 
    - regret_maximizing
    - ${TASK_NAME}
  mode: online # options: online, offline, disabled
  verbose: true
  log_train_out: false # whether to log the out dictionary
  log_video: true # whether to log the video

# Local logger
local_logger:
  save_figures: false # currently unused
  save_train_out: false
  save_video: false # whether to save the video locally