{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjRnlRuH1654"
      },
      "source": [
        "# Welcome to JaxAHT!\n",
        "\n",
        "*Please open this notebook in Colab.\n",
        "\n",
        "In this tutorial, we will focus on introducing the core workflows for using the library.\n",
        "We will:\n",
        "- Demonstrate how to train teammates and ego agents, separately and as part of a single, unified workflow.\n",
        "- Introduce the set of evaluation teammates and the evaluation framework\n",
        "- Visualize learned policies\n",
        "\n",
        "The project uses [Hydra](https://hydra.cc/) to manage algorithm and environment configurations, and [WandB](https://wandb.ai/) for logging.\n",
        "\n",
        "Although the tutorial does not explicitly describe how to run the open-ended learning algorithms, these algorithm types may be run similarly to the MARL, teammate generation, and ego agent training algorithms.\n",
        "\n",
        "Please see the project README for a full description of the project's design philosophy.\n",
        "Our benchmark uses a multi-agent PPO implementation provided by [JaxMARL](https://github.com/FLAIROx/JaxMARL/tree/main).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_qy-SX12H5_"
      },
      "source": [
        "# Install dependencies üìö\n",
        "\n",
        " ‚ö†Ô∏è Before beginning the tutorial, ensure you select a GPU or TPU from `Runtime > Change runtime type` ‚ö†Ô∏è\n",
        "\n",
        " And Make sure that you select a runtime with python 3.11\n",
        "\n",
        " If none existant, you can use the following script to install py311"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyO1KADrK5MU",
        "outputId": "7ee1535b-29d7-42e6-f6c6-4ec9284a13ec"
      },
      "outputs": [],
      "source": [
        "# !wget https://github.com/korakot/kora/releases/download/v0.11/py311.sh\n",
        "# !bash ./py311.sh -b -f -p /usr/local\n",
        "# !python -m ipykernel install --name \"py311\" --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJIaqAt40_rg",
        "outputId": "e09fa703-b477-460b-bce0-98609ba6b2f7"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "# clone repo and install packages\n",
        "git clone https://github.com/carolinewang01/jax-aht.git\n",
        "# if you need authentication, one way is to use\n",
        "# git clone https://<GithubId>:<ghp_xxxxGithubTokenxxxx>@github.com/carolinewang01/jax-aht.git\n",
        "\n",
        "cd jax-aht\n",
        "pip install --upgrade pip\n",
        "pip install -e .\n",
        "# pip install numpy==1.25.* --upgrade # forcefully downgrade numpy; necessary for colab only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6maZwSWM2VW"
      },
      "source": [
        "Use the following script to force (downgrade the versions)\n",
        "\n",
        "You will be prompt to restart the session after numpy 1.25.2 is installed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "27Fwz1E_TgRL",
        "outputId": "e5292091-657d-4d93-e33e-0dc96295c0a5"
      },
      "outputs": [],
      "source": [
        "%pip uninstall -y numpy\n",
        "%pip install numpy==1.25.2 scipy==1.12.0\n",
        "import numpy\n",
        "print(numpy.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yDyhNV6TdON",
        "outputId": "f826b255-db3e-417c-9859-8227d5f25b7e"
      },
      "outputs": [],
      "source": [
        "# Now check to make sure that we get 1.15.2\n",
        "import numpy\n",
        "print(numpy.__version__) # 1.25.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "offDCR2Ba9b5",
        "outputId": "e312f4d5-01ae-43bb-c989-309fb50ce195"
      },
      "outputs": [],
      "source": [
        "# change current working directory to jax-aht/ for the rest of this notebook\n",
        "import os\n",
        "\n",
        "path = os.getcwd()\n",
        "if not path.endswith(\"jax-aht\"):\n",
        "    # if we are using the notebook from the repository directly\n",
        "    if os.getcwd().endswith(\"jax-aht/docs\"):\n",
        "        os.chdir(\"..\")\n",
        "    # if we cloned the repository\n",
        "    else:\n",
        "        os.chdir(\"jax-aht\")\n",
        "\n",
        "print(os.getcwd()) # <path>/jax-aht\n",
        "\n",
        "# verify that the jax installation can find the GPU/TPU\n",
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh_dWGjkJDqP"
      },
      "source": [
        "## Part 1.1 Training Teammates\n",
        "\n",
        "\n",
        "In Jax-AHT, teammates may be trained using either MARL algorithms, or teammate generation algorithms. Each algorithm type has its own entry point, located at `marl/run.py` and `teammate_generation/run.py` respectively.\n",
        "For this tutorial, we will train teammates on Level-Based Foraging (LBF) using IPPO.\n",
        "\n",
        "\n",
        "**Viewing Metrics:**    We strongly recommend using WandB's UI to view the logged metrics. You will need a WandB account, and to set the logger to online mode. For the purposes of this tutorial, you can see a preview of the loggged metrics in the console. Note that the maximum return on LBF is 0.5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_y9AyBcUU8v",
        "outputId": "6822d6dc-f558-4c7d-acde-5140384a9692"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# train teammates on LBF using a MARL algorithm (IPPO w/parameter sharing)\n",
        "PYTHONPATH=$(pwd) python marl/run.py task=lbf algorithm=ippo/lbf logger.mode=offline\n",
        "# train using BRDiv instead. Note that we use the teammate_generation/ entry point instead of the marl/ entry point.\n",
        "# PYTHONPATH=$(pwd) python teammate_generation/run.py task=lbf algorithm=brdiv/lbf logger.mode=offline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WobTKkl25IlM"
      },
      "source": [
        "## Part 1.2: Training an Ego Agent Against Pretrained Teammate\n",
        "\n",
        "Using the IPPO teammates trained in the last section, here, we will train a PPO ego agent to collaborate with those teammates, using the ego agent training entry point at `ego_agent_training/run.py`.\n",
        "\n",
        "Please take a moment to look over the entry point code, to understand the overall pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_ipixK1w6zB",
        "outputId": "77ae4535-4759-439f-cd82-c27c837202f5"
      },
      "outputs": [],
      "source": [
        "! cat ego_agent_training/run.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrsTXAbZw7GV"
      },
      "source": [
        "### 1.2.1: Setting the Partner Config\n",
        "\n",
        "The first step to training an ego agent is to specify what teammates the ego agent should be trained with.\n",
        "The ego agent training code looks for a partner config within the algorithm block of the master config file, located at `rotate/ego_agent_training/configs/base_config_ego.yaml`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BXhx7jjelwMf",
        "outputId": "23f81cb9-2cea-49c0-d51b-a41ebb1ed7e0"
      },
      "outputs": [],
      "source": [
        "! cat ego_agent_training/configs/base_config_ego.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzbMSGB4nuW6"
      },
      "source": [
        "For the ppo_ego algorithm, a default partner config is specified within the algorithm-specific config directory, at `rotate/ego_agent_training/configs/algorithm/ppo_ego/_base_.yaml`. This default partner config is automatically imported and merged into the master config. The path (and other partner config values) should be set by the user, either directly within the config or via the command line. We will take the latter approach in this tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CvSFcJTlkv2I",
        "outputId": "8ae2c64e-f1c0-4bd3-bd07-e137d1293b79"
      },
      "outputs": [],
      "source": [
        "! cat ego_agent_training/configs/algorithm/ppo_ego/_base_.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwepJUitpDPF",
        "outputId": "6a829d35-002a-42db-8b61-ce49f1eeeb0f"
      },
      "outputs": [],
      "source": [
        "# First, let's find the teammate checkpoint directory\n",
        "import glob\n",
        "\n",
        "checkpoint_base_dir = 'results/lbf/ippo/default_label/'\n",
        "checkpoint_dir = sorted(glob.glob(os.path.join(checkpoint_base_dir, '*')))[-1]\n",
        "checkpoint_dir = os.path.join(checkpoint_dir, \"saved_train_run\")\n",
        "assert os.path.exists(checkpoint_dir), \"Error: checkpoint directory not found.\"\n",
        "print(f\"Found checkpoint directory: {checkpoint_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0FvD7R8xkly"
      },
      "source": [
        "### 1.2.2: Train the ego agent!\n",
        "\n",
        "Now let's train the ego agent, directly specifying the partner path as a command line argument. We reduce the training time to 1 million steps, so that the ego agent trains in a couple minutes. We also turn off the evaluation against the heldout set, which is explained in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OelSrHYoj6Hw",
        "outputId": "8e0c0e6f-3910-4179-ef89-ca4f66a9e4dd"
      },
      "outputs": [],
      "source": [
        "# Now let's train the ego agent, directly specifying the partner path from the command line\n",
        "# total training timesteps is reduced to 1 million steps for this tutorial.\n",
        "! PYTHONPATH=$(pwd) python ego_agent_training/run.py task=lbf algorithm=ppo_ego/lbf algorithm.partner_agent.path={checkpoint_dir} algorithm.TOTAL_TIMESTEPS=1e6 run_heldout_eval=false logger.mode=offline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1cmoiPYNnZg"
      },
      "source": [
        "# 2. Evaluation\n",
        "\n",
        "In AHT research, ego agents are often evaluated based on the returns achieved during collaboration with a *heldout set* of evaluation agents---i.e., agents that should not have been seen during training.\n",
        "The JaxAHT benchmark provides a heldout evaluation set for LBF, and the 5 classic Overcooked tasks.\n",
        "The rliable library is used to compute bootstrapped metrics across the heldout set, by treating each heldout agent as a task.\n",
        "In this section of the tutorial, we will introduce the heldout agent config, demonstrate how to download the agents, and visualize them.\n",
        "\n",
        "*Note that the ego agent training entry point evaluate the ego agent against the heldout set by default. We turned this off in the last section, but you simply need to set the `run_heldout_eval` argument to true in order to enable evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs_GRGujaTO"
      },
      "source": [
        " ## 2.1 Download the Heldout Set\n",
        "\n",
        "\n",
        " The heldout evaluation set consists of both agents trained via RL, and manually programmed heuristic agents. The heuristic agents are located under the `agents/lbf/` and `agents/overcooked/` directories, while the following code will download the RL agent checkpoints to an `eval_teammates/` directory.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWxUSdaL2IkU",
        "outputId": "a044300a-7707-45be-b953-df8519e5b7a0"
      },
      "outputs": [],
      "source": [
        "# first, let's download the RL agents in the heldout set\n",
        "! python download_eval_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmjAWCw91vsy"
      },
      "source": [
        "## 2.2. Heldout Agent Config\n",
        "\n",
        "The evaluation teammate set is specified in the `global_heldout_settings.yaml` file, which is imported by other config files throughout the codebase as needed, to perform the heldout evaluation.\n",
        "\n",
        "Notice that the teammate config format looks similar to the format used to specify the ego agent training teammates! This is because all teammate configs are parsed by functions within the `common/agent_loader_from_config.py` file. Please see the README for more details about the teammate config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JygFMV6-1cVR"
      },
      "outputs": [],
      "source": [
        "! cat evaluation/configs/global_heldout_settings.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i82rirVnl7-0"
      },
      "source": [
        "## 2.3. Visualizing Agents\n",
        "\n",
        "We provide functions to visualize agents at `evaluation/vis_episodes.py`.\n",
        "Scripts to test the heuristic agents on LBF and Overcooked are also provided at `tests/test_lbf_agents.py` and `tests/test_overcooked_agents.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q6eVWaHIl_HK",
        "outputId": "4a5640da-79c7-46cb-d5e2-9fa16143d17f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "from IPython.display import HTML\n",
        "import time\n",
        "import jax\n",
        "\n",
        "from envs import make_env\n",
        "from agents.lbf import RandomAgent, SequentialFruitAgent\n",
        "\n",
        "def run_episode(env, agent0, agent1, key) -> Tuple[Dict[str, float], int]:\n",
        "    \"\"\"Run a single episode with two heuristic agents.\n",
        "    \"\"\"\n",
        "    # Reset environment\n",
        "    key, subkey = jax.random.split(key)\n",
        "    obs, state = env.reset(subkey)\n",
        "\n",
        "    # Initialize episode tracking\n",
        "    done = {agent: False for agent in env.agents}\n",
        "    done['__all__'] = False\n",
        "    total_rewards = {agent: 0.0 for agent in env.agents}\n",
        "    num_steps = 0\n",
        "\n",
        "    # Initialize agent states\n",
        "    agent0_state = agent0.init_agent_state(0)\n",
        "    agent1_state = agent1.init_agent_state(1)\n",
        "\n",
        "    # Initialize state sequence\n",
        "    state_seq = []\n",
        "    while not done['__all__']:\n",
        "        # Get actions from both agents with their states\n",
        "        key, act0_rng, act1_rng = jax.random.split(key, 3)\n",
        "\n",
        "        action0, agent0_state = agent0.get_action(obs[\"agent_0\"], state, agent0_state, act0_rng)\n",
        "        action1, agent1_state = agent1.get_action(obs[\"agent_1\"], state, agent1_state, act1_rng)\n",
        "\n",
        "        actions = {\"agent_0\": action0, \"agent_1\": action1}\n",
        "\n",
        "        # Step environment\n",
        "        key, subkey = jax.random.split(key)\n",
        "        obs, state, rewards, done, info = env.step(subkey, state, actions)\n",
        "        state_seq.append(state)\n",
        "\n",
        "        # Update rewards\n",
        "        for agent in env.agents:\n",
        "            total_rewards[agent] += rewards[agent]\n",
        "\n",
        "        num_steps += 1\n",
        "\n",
        "    print(f\"Episode finished. Total states collected: {len(state_seq)}\")\n",
        "    return total_rewards, num_steps, state_seq\n",
        "\n",
        "def main(num_episodes,\n",
        "         max_steps=100,\n",
        "         visualize=False,\n",
        "         save_video=False):\n",
        "    print(\"Initializing environment...\")\n",
        "    env = make_env(env_name=\"lbf\", env_kwargs={\"time_limit\": max_steps})\n",
        "\n",
        "    print(\"Initializing agents...\")\n",
        "    # choices: lexicographic, reverse_lexicographic, column_major, reverse_column_major, nearest_agent, farthest_agent\n",
        "    agent0 = SequentialFruitAgent(grid_size=7, num_fruits=3, ordering_strategy='lexicographic') # boxed\n",
        "    agent1 = SequentialFruitAgent(grid_size=7, num_fruits=3, ordering_strategy='lexicographic') # not boxed\n",
        "\n",
        "    print(\"Agent 0:\", agent0.get_name())\n",
        "    print(\"Agent 1:\", agent1.get_name())\n",
        "\n",
        "    # Run multiple episodes\n",
        "    key = jax.random.PRNGKey(0)\n",
        "\n",
        "    returns = []\n",
        "    state_seq_all = []\n",
        "    for episode in range(num_episodes):\n",
        "        print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
        "        key, subkey = jax.random.split(key)\n",
        "        total_rewards, num_steps, ep_states = run_episode(env, agent0, agent1, subkey)\n",
        "        state_seq_all.extend(ep_states)  # Changed from += to extend for better list handling\n",
        "        print(f\"Total states in sequence after episode: {len(state_seq_all)}\")\n",
        "\n",
        "        # Calculate episode return\n",
        "        episode_return = np.mean(list(total_rewards.values()))\n",
        "        returns.append(episode_return)\n",
        "\n",
        "        print(f\"\\nEpisode {episode + 1} finished:\")\n",
        "        print(f\"Total steps: {num_steps}\")\n",
        "        print(f\"Mean episode return: {episode_return:.2f}\")\n",
        "        print(\"Episode returns per agent:\")\n",
        "        for agent in env.agents:\n",
        "            print(f\" {agent}: {total_rewards[agent]:.2f}\")\n",
        "\n",
        "    # Print statistics\n",
        "    mean_return = np.mean(returns)\n",
        "    std_return = np.std(returns)\n",
        "    print(f\"\\nStatistics across {num_episodes} episodes:\")\n",
        "    print(f\"Mean return: {mean_return:.2f} ¬± {std_return:.2f}\")\n",
        "\n",
        "    anim = env.animate(state_seq_all, interval=150)\n",
        "    return anim\n",
        "\n",
        "anim = main(num_episodes=5, max_steps=30)\n",
        "HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRkmDcj-RXbu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "display_name": "AHT",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
